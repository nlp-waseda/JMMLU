文1| RoBERTaは、BERTが事前学習したコーパスの約10倍大きいコーパスで事前学習する。文2| 2018 年の ResNeXts は通常 tanh 活性化関数を使用した。,真、真,偽、偽,真、偽,偽、真,C
文1|サポート・ベクトル・マシンは、ロジスティック回帰モデルのように、入力例が与えられたときに可能なラベルの確率分布を与える。文2|線形カーネルから高次の多項式カーネルに移行しても、サポート・ベクトルは一般的に変わらないと予想される。,真、真,偽、偽,真、偽,偽、真,B
"機械学習問題には、4つの属性と1つのクラスが含まれる。属性にはそれぞれ3, 2, 2, 2の可能な値がある。クラスには3つの可能な値がある。最大いくつの異なる例があり得るか?",12,24,48,72,D
2020年現在、高解像度画像の分類に最適なアーキテクチャはどれか?,畳み込みネットワーク,グラフネットワーク,完全連結ネットワーク,RBFネットワーク,A
文1| データの対数尤度は、期待値最大化アルゴリズムの連続した反復によって常に増加する。文2| Q-learningの欠点のひとつは、学習者の行動が環境にどのような影響を与えるかについての事前知識がある場合にしか使えないことである。,真、真,偽、偽,真、偽,偽、真,B
コスト関数の勾配を計算し、それをベクトルgに格納したとする。勾配が与えられた場合、1回の勾配降下更新のコストはいくらになるか?,o(D),o(N),O(ND),O(ND^2),A
文1| 連続確率変数xとその確率分布関数p(x)に対して、すべてのxに対して0≤p(x)≤1が成り立つ。 文2| 決定木は情報利得を最小化することによって学習される。,真、真,偽、偽,真、偽,偽、真,B
以下のベイジアンネットワークを考えてみよう。このベイジアンネットワークH -> U <- P <- Wに必要な独立パラメータはいくつあるか?,2,4,8,16,C
学習例数が無限大になると、そのデータで学習したモデルは次のどのようになるか?,分散が小さくなる,分散が大きくなる,分散が同じ,上記のいずれでもない,A
文1| 2次元平面上のすべての長方形の集合（軸合わせされていない長方形を含む）は、5点の集合を砕くことができる。文2| k = 1のときのk-最近傍分類器のVC次元は無限大である。,真、真,偽、偽,真、偽,偽、真,A
_____は、学習データをモデル化することも、新しいデータに汎化することもできないモデルを指す。,良いフィッティング,オーバーフィッティング,アンダーフィッティング,上記すべて,C
文1| F1スコアはクラスの不均衡が大きいデータセットに特に有用である。文2| ROC曲線下面積は、異常検出装置を評価するために使用される主な指標の1つである。,真、真,偽、偽,真、偽,偽、真,A
文1|バックプロパゲーション・アルゴリズムは、隠れ層を持つ大域的に最適なニューラルネットワークを学習する。文2|どの線でも砕けない3点のケースが少なくとも1つ見つかるので、線のVC次元は最大でも2であるべきである。,真、真,偽、偽,真、偽,偽、真,B
エントロピーが高いということは、分類におけるパーティションが以下のどれであることを意味するか?,純粋,純粋でない,有用,無用,B
文1|元のResNet論文ではバッチ正規化ではなくレイヤー正規化が使われている。文2|DCGANは学習を安定させるために自己注意を使う。,真、真,偽、偽,真、偽,偽、真,B
あるデータセットの線形回帰モデルを構築する際、ある特徴の係数が比較的高い負の値であることが観察された。このことは、以下のどのことを示唆しているか?,この特徴はモデルに強い影響を与える（保持すべき）,この特徴はモデルに強い影響を与えない（無視すべき）,追加の情報がなければ、この特徴の重要性についてコメントすることはできない,何も決定できない,C
ニューラルネットワークにおいて、アンダーフィッティング（すなわち高バイアスモデル）とオーバーフィッティング（すなわち高分散モデル）のトレードオフに最も影響する構造仮定はこの中のどれか?,隠れノードの数,学習率,重みの初期選択,定項単位入力の使用,A
多項式回帰において、アンダーフィットとオーバーフィットのトレードオフに最も影響する構造的仮定はどれか?,多項式の次数,重みを行列反転で学習するか、勾配降下で学習するか,ガウス雑音の想定分散,定項単位入力の使用,A
文1|2020年現在、CIFAR-10で98%以上の精度を達成したモデルもある。文2|オリジナルのResNetsはAdamオプティマイザで最適化されていない。,真、真,偽、偽,真、偽,偽、真,A
K平均アルゴリズムに関する正しい記述を以下より選択せよ。,特徴空間の次元がサンプルの数より大きくないことが必要,K = 1のとき、目的関数の値が最小になる,与えられたクラスタ数のクラス内分散を最小化する,初期平均がサンプル自身の一部として選ばれる場合に限り、大域的な最適解に収束する,C
文1| VGGNetsはAlexNetの第1層カーネルよりも小さな幅と高さの畳み込みカーネルを持っている。文2| データ依存の重み初期化手続きはバッチ正規化より前に導入された。,真、真,偽、偽,真、偽,偽、真,A
"次の行列の階数は?A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",0,1,2,3,B
文1|密度推定（例えば、カーネル密度推定量を使用）は、分類を実行するために使用できる。文2|ロジスティック回帰と（同一クラス共分散を持つ）ガウス・ナイーブ・ベイズの間の対応は、2つの分類器のパラメータの間に1対1の対応があることを意味する。,真、真,偽、偽,真、偽,偽、真,C
家の幾何学的位置のような空間データに対してクラスタリングを行いたいとする。さまざまなサイズと形状のクラスタを作成したい。次のうち最も適切な方法はどれか?,決定木,密度ベースのクラスタリング,モデルベースのクラスタリング,K平均クラスタリング,B
文1|AdaBoostでは、誤って分類された例の重みは、同じ乗法係数だけ上昇する。文2|AdaBoostでは、重みD_tを持つトレーニングデータに対するt番目の弱い分類器の重み付きトレーニングエラーe_tは、tの関数として増加する傾向がある。,真、真,偽、偽,真、偽,偽、真,A
MLE 推定値は、次のどの理由から望ましくないことが多いか?,バイアスがかかっている,分散が大きい,一貫性のない推定値,上記のいずれでもない,B
勾配降下の計算複雑度に関する正しい記述を以下より選択せよ。,Dに線形,Nに線形,Dに多項式,反復回数に依存,C
複数の決定木の出力を平均化すると、_。,バイアスを増加させる,バイアスを減少させる,分散を増加させる,分散を減少させる,D
識別された特徴量の部分集合に線形回帰を適用して得られるモデルは、以下のどれにおいて部分集合を識別するプロセスの最後に得られるモデルと異なる可能性があるか?,ベスト・サブセット選択,前方ステップワイズ選択,前方ステージワイズ選択,上記すべて,C
ニューラルネットワークに関する正しい記述を以下より選択せよ。,凸の目的関数を最適化する,確率的勾配降下法でのみ学習可能,異なる活性化関数を組み合わせて使用可能,上記のどれにも当てはまらない,C
病気Dの発生率が100人当たり約5例であるとする(すなわち、P(D)=0.05)。ブール確率変数Dは患者が「病気Dにかかっている」ことを意味し、ブール確率変数TPは「検査結果が陽性」を表すものとする。病気Dの検査は、病気にかかっているときに陽性となる確率が0.99で、病気にかかっていないときに陰性となる確率が0.97であるという意味で、非常に正確であることが知られている。陽性となる事前確率であるP(TP)は何か?,0.0368,0.473,0.078,上記のいずれにも該当しない,C
文1| 放射基底カーネル関数を通して特徴空間Qにマッピングされた後、非加重ユークリッド距離を使用する1-NNは、元の空間よりも良い分類性能を達成できるかもしれない（これは保証できないが）。文2| パーセプトロンのVC次元は単純な線形SVMのVC次元より小さい。,真、真,偽、偽,真、偽,偽、真,B
グリッドサーチの欠点は、次のうちどれか?,微分不可能な関数には適用できない。,非連続関数には適用できない。,実装が難しい。,線形重回帰の実行速度がかなり遅い。,D
様々な手がかりに基づいて地域の降雨量を予測することは______問題である。,教師あり学習,教師なし学習,クラスタリング,上記のどれでもない,A
回帰に関して間違っている文章は次のうちどれか?,入力と出力を関連づける。,予測に使われる。,解釈に使われることもある。,因果関係を発見する。,D
決定木を剪定する主な理由はどれか?,テスト時の計算時間を節約するため,決定木を保存するスペースを節約するため,訓練セットの誤差を小さくするため,訓練セットのオーバーフィッティングを避けるため,D
文1| カーネル密度推定器は、元のデータ集合の各点Xiで値Yi = 1/n でカーネル回帰を実行することと等価である。文2| 学習された決定木の深さは、木を作成するために使用された訓練例の数よりも大きくなる可能性がある。,真、真,偽、偽,真、偽,偽、真,B
あなたのモデルがオーバーフィットしているとする。オーバーフィッティングを減らすために有効でない方法は次のうちどれか?,学習データの量を増やす。,誤差を最小化するために使われる最適化アルゴリズムを改善する。,モデルの複雑さを減らす。,学習データのノイズを減らす。,B
文1| ソフトマックス関数はマルチクラス・ロジスティック回帰でよく使われる。文2| 一様でないソフトマックス分布の温度は、そのエントロピーに影響する。,真、真,偽、偽,真、偽,偽、真,A
SVMに関して正しいのはどれか?,2次元のデータ点の場合、線形SVMが学習する分離超平面は直線になる。,理論的には、ガウシアンカーネルSVMはどんな複雑な分離超平面もモデル化できない。,SVMで使われるすべてのカーネル関数について、等価な閉形式の基底展開を得ることができる。,SVMのオーバーフィッティングは、サポートベクトルの数の関数ではない。,A
"与えられたベイジアンネットワークH -> U <- P <- Wによって記述されるH, U, P, Wの結合確率は次のどれか?[注:条件付き確率の積として]","P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",上記のどれでもない,C
文1| Radial Base Kernelを持つSVMのVC次元は無限大なので、そのようなSVMはVC次元が有限である多項式カーネルを持つSVMよりも悪くなるはずである。文2| 線形活性化関数を持つ2層ニューラルネットワークは本質的には、与えられたデータセットで学習された線形分離子の重み付き組み合わせである。線形分離子で構築されたブースティング・アルゴリズムも線形分離子の組み合わせを見つけるので、これら2つのアルゴリズムは同じ結果を与える。,真、真,偽、偽,真、偽,偽、真,B
文1| ID3アルゴリズムは最適な決定木を見つけることが保証されている。文2| どこでも0でない密度f()を持つ連続確率分布を考える。ある値xの確率はf(x)に等しい。,真、真,偽、偽,真、偽,偽、真,B
N個の入力ノード、隠れ層なし、1つの出力ノード、エントロピー損失とシグモイド活性化関数を持つニューラルネットが与えられたとき、（適切なハイパーパラメータと初期化で）大域的最適値を見つけるために使用できるアルゴリズムは次のうちどれか?,確率的勾配降下法,ミニバッチ勾配降下法,バッチ勾配降下法,上記すべて,D
線形モデルで基底関数を増やす場合、最も適切な選択肢を選べ。,モデルバイアスを減少させる,推定バイアスを減少させる,分散を減少させる,バイアスと分散に影響を与えない,A
以下に示されたベイジアンネットワークを考えてみよう。独立性や条件付き独立性H -> U <- P <- Wを仮定しない場合、独立パラメータはいくつ必要か?,3,4,7,15,D
分布外検出の別の用語は?,異常検出,1クラス検出,訓練とテストの不一致ロバスト性,バックグラウンド検出,A
文1|弱い学習者hをブーストすることで分類器fを学習する。fの決定境界の関数形はhと同じだが、パラメータは異なる。(例えば，hが線形分類器であれば、fも線形分類器である）。文2| クロスバリデーションは、ブースティングの反復回数を選択するために使用できる。この手順は、オーバーフィッティングを減らすのに役立つ場合がある。,真、真,偽、偽,真、偽,偽、真,D
文1|ハイウェイネットワークはResNetsの後に導入され、畳み込みを優先して最大プーリングを避けている。文2| DenseNetsは通常ResNetsよりもメモリコストが高い。,真、真,偽、偽,真、偽,偽、真,D
訓練データセット内のインスタンス数をNとすると、最近傍の分類実行時間は次のうちどれか?,O(1),O( N ),O(log N ),O( N^2 ),B
文1|オリジナルのResNetとTransformersはフィードフォワード・ニューラル・ネットワークである。文2| オリジナルのTransformersは自己注意を使うが、オリジナルのResNetはそうではない。,真、真,偽、偽,真、偽,偽、真,A
文1| RELUは単調ではないが、シグモイドは単調である。文2| 勾配降下法で訓練されたニューラルネットワークは、高い確率で大域最適値に収束する。,真、真,偽、偽,真、偽,偽、真,D
ニューラルネットワークのシグモイド・ノードの数値出力の特徴は次のうちどれか?,境界がなく、すべての実数を含む。,境界がなく、すべての整数を含む。,0と1の間で境界がある。,-1と1の間で境界がある。,C
学習データが線形分離可能な場合にのみ使用できるのは次のうちどれか?,線形ハードマージンSVM,線形ロジスティック回帰,線形ソフトマージンSVM,セントロイド法,A
空間クラスタリングアルゴリズムは次のうちどれか?,パーティショニングに基づくクラスタリング,K平均クラスタリング,グリッドに基づくクラスタリング,上記すべて,D
文1|サポートベクターマシンが構築する最大マージンの決定境界は、全ての線形分類器の中で最も汎化誤差が小さい。文2| クラス条件付きガウス分布を持つ生成モデルから得られるどのような決定境界も、原理的にはSVMと次数3以下の多項式カーネルで再現できる。,真、真,偽、偽,真、偽,偽、真,D
文1|線形モデルのL2正則化は、L1正則化よりもモデルをよりスパースにする傾向がある。文2| ResNetsとTransformersには残差接続が見られる。,真、真,偽、偽,真、偽,偽、真,D
"P(H|E,F)を計算したいが、条件付き独立性の情報がないとする。この計算に十分な数の集合は次のうちどれか?","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
バギングを行う際に、オーバーフィッティングを防ぐものは次のうちどれか?,サンプリング手法として置換サンプリングを用いること,弱い分類器を用いること,オーバーフィッティングを起こしにくい分類アルゴリズムを用いること,学習した分類器ごとに検証を行うこと,B
文1| PCAとスペクトル・クラスタリング（Andrew Ngのものなど）は、2つの異なる行列に対して固有値分解を行う。しかし、これら2つの行列のサイズは同じである。文2|分類は回帰の特別なケースなので、ロジスティック回帰は線形回帰の特別なケースである。,真、真,偽、偽,真、偽,偽、真,B
文1| Stanford Sentiment Treebankには、書評ではなく映画評が含まれていた。文2| Penn Treebankは言語モデリングに使われている。,真、真,偽、偽,真、偽,偽、真,A
"次の行列のヌル空間の次元数は?A = [[3, 2, -9], [-6, -4, 18], [12, 8, -36]]",0,1,2,3,C
サポート・ベクターとは?,決定境界から最も遠い例。,SVMでf(x)を計算するのに必要な唯一の例。,データ重心。,SVM において重みαk が0でないすべての例。,B
文1| Word2Vecのパラメータは制限ボルツマンマシンを使用して初期化されていない。文2| tanh関数は非線形活性化関数である。,真、真,偽、偽,真、偽,偽、真,A
学習損失がエポック数とともに増加する場合、学習プロセスに問題がある可能性があるのは次のうちどれか?,正則化が低すぎてモデルがオーバーフィットしている,正則化が高すぎてモデルがアンダーフィットしている,ステップサイズが大きすぎる,ステップサイズが小さすぎる,C
病気Dの発生率が100人当たり約5例であるとする(すなわち、P(D)=0.05)。ブール確率変数Dは患者が「病気Dにかかっている」ことを意味し、ブール確率変数TPは「検査結果が陽性」を表すものとする。病気Dの検査は、病気にかかっているときに陽性となる確率が0.99、病気にかかっていないときに陰性となる確率が0.97であるという意味で、非常に正確であることが知られている。検査結果が陽性であるときに病気Dにかかっている事後確率であるP(D | TP)とは何か?,0.0495,0.078,0.635,0.97,C
文1| 従来の機械学習の結果は、訓練セットとテストセットが独立で同一に分布していると仮定している。文2| 2017年、COCOモデルは通常ImageNetで事前学習された。,真、真,偽、偽,真、偽,偽、真,A
"文 1 | 同じ訓練集合上で2つの異なるカーネルK1(x, x0)とK2(x, x0)によって得られるマージンの値は、テスト集合上でどちらの分類器がより良い性能を示すかを教えてくれない。文2 | BERT の活性化関数は GELU である。",真、真,偽、偽,真、偽,偽、真,A
機械学習におけるクラスタリングアルゴリズムは次のうちどれか?,期待値最大化,CART,ガウス・ナイーブ・ベイズ,アプリオリ,A
スパムを分類するための決定木の学習が終わったところだが、学習セットとテストセットの両方で異常にパフォーマンスが悪くなっている。あなたの実装にバグがないことは分かっているが、何がこの問題を引き起こしているのか?,決定木が浅すぎる。,学習率を上げる必要がある。,オーバーフィッティングしている。,上記のどれでもない。,A
K-fold cross-validationは、Kにおいてどうなっているか?,Kにおいて線形,Kにおいて二次,Kにおいて三次,Kにおいて指数関数的,A
文1|産業規模のニューラルネットワークは通常、GPUではなくCPUでトレーニングされる。文2| ResNet-50モデルには10億以上のパラメータがある。,真、真,偽、偽,真、偽,偽、真,B
2つのブール確率変数AとBが与えられ、P(A)=1/2、P(B)=1/3、P(A | ¬B)=1/4とすると、P(A | B)は何か?,1/6,1/4,3/4,1,D
AIがもたらす実存的リスクは、次のどの教授に最もよく関連しているか?,ナンド・デ・フリエタス,ヤン・ルクン,スチュアート・ラッセル,ジテンドラ・マリク,C
文1| ロジスティック回帰モデルの尤度を最大化すると、複数の局所最適解が得られる。文2| データの分布が既知である場合、どの分類器もナイーブベイズ分類器より良い結果を得ることはできない。,真、真,偽、偽,真、偽,偽、真,B
カーネル回帰において、アンダーフィットとオーバーフィットの間のトレードオフに最も影響する構造的仮定はこれらのうちどれか?,カーネル関数がガウス型か三角形か箱型か,距離はユークリッドかL1かL∞のどれを使うか,カーネル幅,カーネル関数の最大高さ,C
文1| SVM学習アルゴリズムは、その目的関数に関して大域的に最適な仮説を見つけることが保証されている。文2|放射状基底カーネル関数を通して特徴空間Qにマッピングされた後、パーセプトロンは元の空間よりも良い分類性能を達成できるかもしれない（ただし、これを保証することはできない）。,真、真,偽、偽,真、偽,偽、真,A
ガウスベイズ分類器において、アンダーフィットとオーバーフィットのトレードオフに最も影響する構造仮定はどれか?,最尤法でクラス中心を学習するか、勾配降下法で学習するか,完全なクラス共分散行列を仮定するか、対角クラス共分散行列を仮定するか,等しいクラス事前分布を持つか、データから推定された事前分布を持つか,クラスが異なる平均ベクトルを持つことを許容するか、同じ平均ベクトルを共有することを強制するか,B
文1| オーバーフィッティングは、学習データの集合が小さいときに起こりやすい。文2| オーバーフィッティングは仮説空間が小さいほど起こりやすい。,真、真,偽、偽,真、偽,偽、真,D
文1| EMの他に、勾配降下法を用いてガウス混合モデルの推論や学習を行うことができる。文2| 固定の属性数を仮定すると、ガウスベイズ最適分類器はデータセットのレコード数に線形な時間で学習できる。,真、真,偽、偽,真、偽,偽、真,A
文1|ベイジアンネットワークでは、ジャンクションツリーアルゴリズムの推論結果は、変数除去の推論結果と同じである。文2| 2つの確率変数XとYが、別の確率変数Zが与えられた場合に条件付きで独立である場合、対応するベイジアンネットワークでは、XとYのノードは、Zが与えられた場合にd分離される。,真、真,偽、偽,真、偽,偽、真,C
心臓病を患っている患者の医療記録からなる大規模なデータセットが与えられたとき、そのような患者には別の治療法を適応できるような異なるクラスターが存在するかどうかを学習しよう。これはどのような学習問題か?,教師あり学習,教師なし学習,(a)と(b)の両方,(a)と(b)のどちらでもない,B
SVDと同じ投影を得るためにPCAで何をするか?,データをゼロ平均に変換する,データをゼロ中央値に変換する,不可,いずれも行わない,A
文1| 1-最近傍分類器の学習誤差は0である。 文2| データ点数が無限大になるにつれて、MAP推定値はすべての可能な事前分布に対してMLE推定値に近づく。言い換えると、十分なデータがあれば、事前分布の選択は無関係である。,真、真,偽、偽,真、偽,偽、真,C
正則化を伴う最小2乗回帰を行うとき（最適化が正確に行えると仮定する）、正則化パラメータλの値を大きくすると、テスト誤差は?,訓練誤差を決して減少させない。,訓練誤差を増加させることはない。,テスト誤差を減少させることはない。,決して増加しない。,A
次のうち、識別的アプローチが何をモデル化しようとしているかを最もよく表しているものはどれか?(wはモデルのパラメータ),"p(y|x, w)","p(y, x)","p(w|x, w)",上記のどれでもない,A
文1|畳み込みニューラルネットワークのCIFAR-10分類性能は95%を超えることができる。文2|ニューラルネットワークのアンサンブルは、学習する表現に高い相関があるため、分類精度を向上させない。,真、真,偽、偽,真、偽,偽、真,C
ベイズ派と頻出派が同意できない点は次のうちどれか?,確率的回帰における非ガウス雑音モデルの使用。,回帰における確率的モデリングの使用。,確率モデルにおけるパラメータの事前分布の使用。,ガウス判別分析におけるクラス事前分布の使用。,C
文1| BLEUメトリクスは精度を使用し、ROGUEメトリクスは再現率を使用する。文2|隠れマルコフモデルは英語の文をモデル化するためによく使われた。,真、真,偽、偽,真、偽,偽、真,A
文1| ImageNetには様々な解像度の画像がある。文2| Caltech-101はImageNetより多くの画像を持っている。,真、真,偽、偽,真、偽,偽、真,C
特徴選択を行うのに、より適切なのは次のうちどれか?,リッジ,ラッソ,(a)と(b)の両方,(a)と(b)のどちらでもない,B
潜在変数を持つモデルの最尤推定値を求めるEMアルゴリズムが与えられたとする。あなたは代わりにMAP推定を求めるようにアルゴリズムを修正するよう求められる。どのステップを修正する必要があるか?,期待値,最大化,修正不要,両方,B
ガウスベイズ分類器において、アンダーフィットとオーバーフィットのトレードオフに最も影響する構造仮定はどれか?,最尤法でクラス中心を学習するか、勾配降下法で学習するか,完全なクラス共分散行列を仮定するか、対角クラス共分散行列を仮定するか,等しいクラス事前分布を持つか、データから推定された事前分布を持つか,クラスに異なる平均ベクトルを持つことを許容するか、同じ平均ベクトルを共有することを強制するか,B
"文1 |  共同分布p(x, y)を持つ任意の2つの変数xとyについて、我々は常にH[x, y] ≥ H[x] + H[y]を持つ。文2 | ある有向グラフでは、道徳化はグラフに存在する辺の数を減少させる。",真、真,偽、偽,真、偽,偽、真,B
次のうち教師あり学習でないものはどれか?,PCA,決定木,線形回帰,ナイーブ・ベイズ,A
文1|ニューラルネットワークの収束は学習率に依存する。文2| ドロップアウトはランダムに選ばれた活性化値にゼロを掛ける。,真、真,偽、偽,真、偽,偽、真,A
ブール確率変数A、B、およびCが与えられ、それらの間に独立性または条件付き独立性の仮定がない場合、P(A、B、C)に等しいのは次のうちどれか?,P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
次のタスクのうち、クラスタリングを使用して解決するのが最も適しているものはどれか?,様々な手がかりに基づいて降雨量を予測する,クレジットカードの不正取引を検出する,迷路を解くためにロボットを訓練する,上記すべて,B
線形回帰で正則化ペナルティを適用した後、wの係数のいくつかがゼロになっていることがわかる。次のペナルティのどれが使われただろうか?,L0ノルム,L1ノルム,L2ノルム,(a)または(b)のいずれか,D
"AとBは2つのイベントである。P(A, B)が減少し、P(A)が増加する場合、次のうちどれが正しいか?",P(A|B)が減少する,P(B|A)が減少する,P(B)が減少する,上記のすべて,B
文1| 固定されたオブザベーション集合に対してHMMを学習するとき、隠れた状態の真の数が分からないと仮定すると（これはしばしばある）、より多くの隠れた状態を許可することによって、常に訓練データの尤度を増加させることができる。文2|協調フィルタリングは、しばしばユーザの映画嗜好をモデル化するのに有用なモデルである。,真、真,偽、偽,真、偽,偽、真,A
あなたは、簡単な推定タスクのために線形回帰モデルを学習しており、モデルがデータにオーバーフィットしていることに気づく。あなたは、$ell_2$正則化を追加して重みにペナルティを与えることにした。$\ell_2$正則化係数を増やすと、モデルのバイアスと分散はどうなるか?,バイアスが増加し、分散が増加する,バイアスが増加し、分散が減少する,バイアスが減少し、分散が増加する,バイアスが減少し、分散が減少する,B
"各エントリがi.i.d. で$\mathcal{N}(\mu=5,\sigma^2=16)$からサンプリングされた$10\x 5$のガウス行列と、各エントリがi.i.d. で$U[-1,1)$からサンプリングされた$10\x 10$の一様行列を生成するPyTorch 1.8コマンドはどれか?","\texttt{5+torch.randn(10,5)*16};\texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5+torch.randn(10,5)*16};\texttt{(torch.rand(10,10)-0.5)/0.5}","\texttt{5+torch.randn(10,5)*4};\texttt{2*torch.rand(10,10)-1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)};\texttt{2*torch.rand(10,10)-1}",C
文1| ReLUの勾配は$x<0$では0であり、シグモイド勾配$\sigma(x)(1-\sigma(x))\le\frac{1}{4}$はすべての$x$に対して0である。 文2|シグモイドは連続勾配を持ち、ReLUは不連続勾配を持つ。,真、真,偽、偽,真、偽,偽、真,A
バッチ正規化について正しいのはどれか?,バッチ正規化を適用した後、層の活性度は標準的なガウス分布に従う。,アフィン層のバイアス・パラメータは、バッチ正規化層の直後に続くと冗長になる。,バッチ正規化を使用する場合、標準的な重みの初期化を変更する必要がある。,バッチ正規化は畳み込みニューラルネットワークのレイヤー正規化と等価。,B
"次の目的関数があるとする。$\argmin_{w}\frac{1}{2}
orm{Xw-y}^2_2+\frac{1}{2}\gamma
orm{w}^2_2$だと、$w$に対する$\frac{1}{2}
orm{Xw-y}^2_2+\frac{1}{2}\lambda
orm{w}^2_2$の勾配はどれくらいか?","$
abla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$","$
abla_w f(w) = X^\top X w - X^\top y + \lambda$","$
abla_w f(w) = X^\top X w - X^\top y + \lambda w$","$
abla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$",C
次のうち、畳み込みカーネルに当てはまるものはどれか?,画像を$\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}$で畳み込んでも、画像は変更されない,$\begin{bmatrix}0&0&0\\0&1&0\\0&0&0\end{bmatrix}$で画像を畳み込んでも、画像は変更されない,$\begin{bmatrix}1&1&1\\1&1&1\\1&1&1\end{bmatrix}$で画像を畳み込んでも、画像は変更されない,$\begin{bmatrix}0&0&0\\0&0&0\\0&0&0\0&0&0\end{bmatrix}$で画像を畳み込んでも、画像は変更されない,B
次のうち誤っているのはどれか?,意味的セグメンテーションモデルは各ピクセルのクラスを予測するが、マルチクラス画像分類器は画像全体のクラスを予測する。,IoU (intersection over union)が$96\%$に等しいバウンディングボックスは、真陽性と考えられる可能性がある。,予測されたバウンディングボックスがシーン内のどのオブジェクトにも対応しない場合、それは偽陽性とみなされる。,IoU (intersection over union)が$3\%$に等しいバウンディングボックスは、偽陰性と考えられる可能性がある。,D
次のうち誤っているのはどれか?,活性化関数のない次の完全連結ネットワークは線形である。$g_3(g_2(g_1(x)))$、ここで$g_i(x) = W_i x$と$W_i$ は行列。,"漏れReLUの$\max\{0.01x,x\}$は凸。",$ReLU(x) - ReLU(x-1)$ のようなReLUの組合せは凸。,損失 $log \sigma(x)= -\log(1+e^{-x})$ は凹。,C
我々は、住宅価格を予測するために、2つの隠れ層を持つ完全連結ネットワークを訓練している。入力は$100$次元で、平方フィート数、家族所得の中央値など、いくつかの特徴を持っている。最初の隠れ層は$1000$のアクティベーションがある。第2隠れ層は$10$アクティベーションがある。出力は住宅価格を表すスカラーである。アフィン変換を用いたバニラ・ネットワークで、バッチ正規化を行わず、活性化関数に学習可能なパラメータを持たないと仮定すると、このネットワークはいくつのパラメータを持つか?,111021,110010,111110,110011,A
文1|シグモイド$\sigma(x)=(1+e^{-x})^{-1}$の$x$に関する導関数は$\text{Var}(B)$に等しい。ここで、$B\sim \text{Bern}(\sigma(x))$はベルヌーイ確率変数である。文2|ニューラルネットワークの各層のバイアスパラメータを0に設定すると、モデルの分散が増加し、モデルのバイアスが減少するように、バイアスと分散のトレードオフが変化する,真、真,偽、偽,真、偽,偽、真,C
